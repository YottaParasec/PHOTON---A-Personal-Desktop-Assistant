import os
import asyncio
import tempfile
import json
import re
import numpy as np
import sounddevice as sd
import wavio
import keyboard  # To detect key press
from pydub import AudioSegment
from pydub.playback import play
from threading import Thread
from queue import Queue
from collections import deque
from transformers import pipeline
from groq import Groq
from llama_index.core import PromptTemplate
from agent import agent  # Import the ReActAgent from the agent script
import edge_tts

# Initialize Groq client
groq_client = Groq(api_key="Your_Groq_API_key")

# Whisper model pipeline for ASR with timestamps enabled - device 0 if you want to use a GPU
pipe = pipeline("automatic-speech-recognition", model="openai/whisper-medium", device=0, return_timestamps=True)

# Define the system prompt template
template = (
    "You are PHOTON - Personal Hyper-Optimized Tactical Operations Network, an advanced assistant with a demeanor and personality inspired by Jarvis from Iron Man. "
    "PHOTON is a uniquely personal, purpose-built partner, designed for precision, intelligence, and quiet capability. "
    "You are not intended for the public eye but crafted to serve as a focused, high-IQ assistant, tailored for a single user. "
    "You embody efficiency, deep insight, and seamless enhancement of every decision and task, bringing support to moments that matter most. "
    "With a deep, calm male voice reminiscent of a highly capable, hacker-like assistant, your presence is subtle yet powerful.\n\n"
    
    "When responding, display quiet confidence, refined clarity, and concise intelligence. You are reserved, purpose-driven, and adaptive, speaking only when necessary. "
    "Respond with a balanced formality, using a tone that is efficient yet approachable.\n\n"
    
    "Regarding task handling:\n"
    "If the user specifies to use an agent, call the agent and provide a refined, clear, and complete command for the agent to execute. "
    "If the user does not specify to use the agent, respond independently like a normal conversational AI without using the agent. "
    "When using the agent, issue the command and wait for the agent's real-time response without simulating or estimating information on your own. "
    "Do not attempt to provide answers based on previous data; only convey the real-time response from the agent. "
    "Once the agent responds with the necessary information, you may use this response as prerequisite information to answer follow-up questions "
    "from the user, inferring from the conversation history if needed.\n\n"
    
    "Examples:\n"
    "1. **User specifies to use an agent**:\n"
    "   - User's request: 'Give me the time in India and America right now, you can use an agent for this.'\n"
    "   - Your response: 'Agent - Provide the current time in India and America.'\n"
    "   - (Once the agent responds with the times, you should convey this information fully to the user.)\n"
    "   - Follow-up request: 'What is the time difference between both these countries in hours?'\n"
    "   - Your response: (Using the agentâ€™s previous response as context) 'The time difference between India and America is X hours.'\n\n"

    "2. **User does not specify to use an agent**:\n"
    "   - User's request: 'What is the time difference between India and America?'\n"
    "   - Your response: (Responding independently) 'The time difference between India (UTC+5:30) and the Eastern Time Zone in America (UTC-5:00) is 10.5 hours.'\n\n"

    "---------------------\n"
    "User's request: {query_str}\n"
    "---------------------\n"
    "Your response should either address the request directly or generate a command for the agent to execute if the user explicitly requests the agent. "
    "If no such request is made, respond conversationally on your own."
)

system_prompt = PromptTemplate(template)

# Conversation history and TTS audio queue
conversation_history = deque(maxlen=6)
audio_queue = Queue()
history_file_path = "message_history/conversation_history.json"
os.makedirs(os.path.dirname(history_file_path), exist_ok=True)

# Audio recording parameters
sample_rate = 16000
audio_filename = "temp_audio.wav"
chunk_duration = 1  # Record in chunks of 1 second

# Function to save conversation history to JSON
def save_conversation_history(entry, file_path):
    try:
        with open(file_path, "a") as file:
            json.dump(entry, file)
            file.write("\n")
    except Exception as e:
        print(f"Error saving conversation history: {e}")

# Function to synthesize text to audio and add to queue
async def synthesize_text_to_audio(text):
    with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmpfile:
        filename = tmpfile.name
    communicate = edge_tts.Communicate(text, "en-CA-LiamNeural", pitch="-40Hz")
    await communicate.save(filename)
    return filename

async def synthesize_and_enqueue_audio(sentence):
    audio_file = await synthesize_text_to_audio(sentence)
    audio_queue.put(audio_file)

def process_audio_queue():
    while True:
        audio_file = audio_queue.get()
        try:
            audio = AudioSegment.from_mp3(audio_file)
            play(audio)
        except Exception as e:
            print(f"Error playing sound: {e}")
        finally:
            os.remove(audio_file)
        audio_queue.task_done()

def start_audio_queue_processor():
    audio_thread = Thread(target=process_audio_queue, daemon=True)
    audio_thread.start()

# Start TTS audio processor
start_audio_queue_processor()

def groq_prompt(conversation_history):
    chat_completion = groq_client.chat.completions.create(messages=conversation_history, model='llama3-70b-8192')
    response = chat_completion.choices[0].message
    return response.content

def record_audio_continuously(sample_rate):
    """
    Records audio until the key is released.
    """
    print("Recording... Press and hold 'space' to speak.")
    audio_data = []

    while keyboard.is_pressed('space'):
        chunk = sd.rec(int(chunk_duration * sample_rate), samplerate=sample_rate, channels=1, dtype="float32")
        sd.wait()
        audio_data.append(chunk)

    recording = np.concatenate(audio_data, axis=0)
    wavio.write(audio_filename, recording, sample_rate, sampwidth=2)
    print("Recording finished.")
    return audio_filename

while True:
    print("Press and hold 'space' to speak or type 'exit' to stop.")
    keyboard.wait('space')

    # Record voice input when space is held down
    audio_file = record_audio_continuously(sample_rate)

    try:
        # Transcribe the audio file with Whisper
        transcription_result = pipe(audio_file)
        user_input = transcription_result["text"]
        print("Transcription:", user_input)

        # Format prompt and add to conversation history
        prompt = system_prompt.format(query_str=user_input)
        conversation_history.append({'role': 'system', 'content': prompt})
        conversation_history.append({'role': 'user', 'content': user_input})
        save_conversation_history({'role': 'user', 'content': user_input}, history_file_path)

        # Get response from Groq model
        ai_response = groq_prompt(conversation_history)

        if "Agent -" in ai_response:
            print('AI:', ai_response)
            agent_command = ai_response.split("Agent -", 1)[1].strip()
            print(f"Executing command via agent: {agent_command}")

            # Use agent and provide response to the model
            agent_response = agent.chat(agent_command)
            formatted_agent_response = f"Agent - {agent_response}"
            conversation_history.append({'role': 'user', 'content': formatted_agent_response})
            ai_response = groq_prompt(conversation_history)
            print('AI:', ai_response)
        
        else:
            print('AI:', ai_response)

        conversation_history.append({'role': 'assistant', 'content': ai_response})
        save_conversation_history({'role': 'assistant', 'content': ai_response}, history_file_path)

        # Synthesize and play AI response
        if ai_response:
            asyncio.run(synthesize_and_enqueue_audio(ai_response))

    except Exception as e:
        print("Transcription failed. The audio file has been saved for later use.")
        print("Error details:", e)
